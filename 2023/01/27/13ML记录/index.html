<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="jerem1ah">
    
    <title>
        
            ML记录-理论基础部分 |
        
        Jerem1ah&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/imgs/50ac4ec68298791a5339fb793e82115.jpg">
    
<link rel="stylesheet" href="/fontawesome/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/regular.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/css/brands.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066cc","logo":"/images/imgs/50ac4ec68298791a5339fb793e82115.jpg","favicon":"/images/imgs/50ac4ec68298791a5339fb793e82115.jpg","avatar":"/images/imgs/50ac4ec68298791a5339fb793e82115.jpg","font_size":null,"font_family":null,"hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"header_transparent":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving.","font_color":null,"hitokoto":false},"scroll":{"progress_bar":true,"percent":true}},"local_search":{"enable":false,"preload":false},"code_copy":{},"code_block":{"tools":{"enable":true,"style":"mac"},"highlight_theme":"obsidian"},"side_tools":{},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":true,"use":"gitalk","valine":{"appid":null,"appkey":null,"placeholder":null},"gitalk":{"github_id":"Lejeremiah","github_admins":null,"repository":"gitalk","client_id":"a415960e8a19902e98bf","client_secret":"97151a67ce9c95ad3c333c402f4cee0f5f05020f"},"twikoo":{"env_id":null,"region":null,"version":"1.6.7"},"waline":{"server_url":null,"reaction":false,"version":2}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"enable":false,"wordcount":false,"min2read":false},"img_align":"left","copyright_info":false},"version":"3.5.2"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/imgs/50ac4ec68298791a5339fb793e82115.jpg">
                </a>
            
            <a class="logo-title" href="/">
               Jerem1ah&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >
                                LINKS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links">LINKS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            <div class="article-title">
                <span class="title-hover-animation">ML记录-理论基础部分</span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/imgs/50ac4ec68298791a5339fb793e82115.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">jerem1ah</span>
                            
                                <span class="author-label">Lv4</span>
                            
                        </div>
                        <div class="meta-info">
                            
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2023-01-27 20:25:06</span>
        <span class="mobile">2023-01-27 20:25</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2024-05-04 17:24:34</span>
    </span>
    
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/ML/">ML</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content keep-markdown-body">
                <h1 id="ML记录-理论基础部分"><a href="#ML记录-理论基础部分" class="headerlink" title="ML记录-理论基础部分"></a>ML记录-理论基础部分</h1><h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>记录自己的学习过程，方便后期回顾复习！</p>
<p>参考：吴恩达机器学习老版视频</p>
<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h2><p>supervised learning algorithm 监督学习算法</p>
<p>regression 回归</p>
<h3 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1.模型介绍"></a>1.模型介绍</h3><p>data set 数据集</p>
<p>m &#x3D; number of training examples</p>
<p>x &#x3D; input variable &#x2F; features</p>
<p>y &#x3D; output variable &#x2F; target variable</p>
<hr>
<p>(x,y) one training example</p>
<p>(x^i,y^i) i^th training example 【上标从1开始】</p>
<hr>
<p>hypothesis 假设函数</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127204332585.png" alt="image-20230127204332585"></p>
<h3 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h3><p>theta_i 模型参数</p>
<p>cost function 代价函数</p>
<p>minimize 最小化</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127205727885.png" alt="image-20230127205727885"></p>
<h3 id="3-代价函数（一）"><a href="#3-代价函数（一）" class="headerlink" title="3.代价函数（一）"></a>3.代价函数（一）</h3><p>review：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127210105461.png" alt="image-20230127210105461"></p>
<hr>
<p>this class:</p>
<p>侧重理解</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127211609902.png" alt="image-20230127211609902"></p>
<h3 id="4-代价函数（二）"><a href="#4-代价函数（二）" class="headerlink" title="4.代价函数（二）"></a>4.代价函数（二）</h3><p>goal:</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127211855438.png" alt="image-20230127211855438"></p>
<p>this class:</p>
<p>侧重理解二元的情况</p>
<h3 id="5-梯度下降"><a href="#5-梯度下降" class="headerlink" title="5.梯度下降"></a>5.梯度下降</h3><p>gradient descent 梯度下降</p>
<p>assigment 赋值</p>
<p>learning rate 学习率 alpha</p>
<p>calculus 微积分</p>
<p>simultaneously update theta0 and theta1 同时更新θ0和θ1</p>
<hr>
<p> start with some theta0 theta1 (say theta0&#x3D;0, theta1&#x3D;0)</p>
<p>class思路：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230127212620071.png" alt="image-20230127212620071"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128111537403.png" alt="image-20230128111537403"></p>
<p>【以上：梯度下降的工作过程，和正确的更新方法】</p>
<h3 id="6-梯度下降知识总结"><a href="#6-梯度下降知识总结" class="headerlink" title="6.梯度下降知识总结"></a>6.梯度下降知识总结</h3><p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128113414659.png" alt="image-20230128113414659"></p>
<p>导数项的正负影响：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128113536395.png" alt="image-20230128113536395"></p>
<p>learning rate的大小影响：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128113707187.png" alt="image-20230128113707187"></p>
<p>fixed learning rate导数项可以自动变化：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128113328178.png" alt="image-20230128113328178"></p>
<h3 id="7-线性回归的梯度下降"><a href="#7-线性回归的梯度下降" class="headerlink" title="7.线性回归的梯度下降"></a>7.线性回归的梯度下降</h3><p>convex function凸函数</p>
<p>bow-shaped functioin弓形函数</p>
<p>global optimum 全局最优</p>
<p>batch :each step of gradient descent uses all the training examples.</p>
<hr>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128142408847.png" alt="image-20230128142408847"></p>
<p>二元的梯度下降求导：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128142746183.png" alt="image-20230128142746183"></p>
<p> batch gradient descent:</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128143637286.png" alt="image-20230128143637286"></p>
<h2 id="2-多元情况（矩阵"><a href="#2-多元情况（矩阵" class="headerlink" title="2.多元情况（矩阵"></a>2.多元情况（矩阵</h2><h3 id="1-多元假设函数"><a href="#1-多元假设函数" class="headerlink" title="1.多元假设函数"></a>1.多元假设函数</h3><p>multiple variable &#x2F; multiple features 多变量 多特征</p>
<p>four dimensional vector 四维向量</p>
<p>theta transpose θ转置</p>
<p>one matrix 一维矩阵</p>
<hr>
<p>notation:</p>
<p>n &#x3D; number of features</p>
<p>x^i &#x3D; input features of i^th training example.</p>
<p>x^i_j &#x3D; value of feature j in i^th training example.</p>
<hr>
<p>多特征下的假设形式：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230128145723949.png" alt="image-20230128145723949"></p>
<p>【x是n+1维、θ也是n+1维、x0&#x3D;1】</p>
<p> multivariate linear regression. 多元线性回归</p>
<h3 id="2-多元梯度下降法"><a href="#2-多元梯度下降法" class="headerlink" title="2.多元梯度下降法"></a>2.多元梯度下降法</h3><p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230129235638695.png" alt="image-20230129235638695"></p>
<h3 id="3-特征缩放-【帮助梯度下降更快的收敛】"><a href="#3-特征缩放-【帮助梯度下降更快的收敛】" class="headerlink" title="3.特征缩放-【帮助梯度下降更快的收敛】"></a>3.特征缩放-【帮助梯度下降更快的收敛】</h3><p>feature scaling 特征缩放</p>
<p>fewer iterations 迭代次数少</p>
<p>converge more quickly 更快的收敛</p>
<p>skewed elliptical shape 歪斜的椭圆形</p>
<p>mean normalization 均值归一化</p>
<p>standard deviation 标准差</p>
<hr>
<p>引例：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130000127723.png" alt="image-20230130000127723"></p>
<p>多少合适、多大可以接受：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130001210088.png" alt="image-20230130001210088"></p>
<p>均值归一化方法介绍：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130001810881.png" alt="image-20230130001810881"></p>
<h3 id="4-学习率"><a href="#4-学习率" class="headerlink" title="4.学习率"></a>4.学习率</h3><p>automatic convergence test 自动收敛测试</p>
<p>small value epsilon ε【阈值】</p>
<hr>
<p>图像可以提示你算法是否正常工作：【use smaller α】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130133509198.png" alt="image-20230130133509198"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130133907888.png" alt="image-20230130133907888"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130133846681.png" alt="image-20230130133846681"></p>
<p>总结-取怎样的学习率合适：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230130134206851.png" alt="image-20230130134206851"></p>
<h3 id="5-特征和多项式回归"><a href="#5-特征和多项式回归" class="headerlink" title="5.特征和多项式回归"></a>5.特征和多项式回归</h3><p>polynomial regression 多项式回归</p>
<p>to use the machinery of linear regression to fit very complicated, even very non-linear functions.</p>
<hr>
<p>定义新的特征的例子：defining new features</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131094605986.png" alt="image-20230131094605986"></p>
<p>多项式回归：【feature scaling becomes increasingly important】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131095201123.png" alt="image-20230131095201123"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131095417451.png" alt="image-20230131095417451"></p>
<p>【如何决定使用什么特征很困难，之后的课程会探讨算法自动选择要使用的特征，来决定是二次还是三次等等】</p>
<h3 id="6-正规方程（区别于迭代法"><a href="#6-正规方程（区别于迭代法" class="headerlink" title="6.正规方程（区别于迭代法"></a>6.正规方程（区别于迭代法</h3><p>normal equation 正规方程</p>
<p>design matrix 设计矩阵</p>
<hr>
<p>例子：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131102545732.png" alt="image-20230131102545732"></p>
<p>构建设计矩阵：【不难】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131103229240.png" alt="image-20230131103229240"></p>
<p>正规方程法：【不需要特征缩放，n小时代价小】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131102354544.png" alt="image-20230131102354544"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131102402736.png" alt="image-20230131102402736"></p>
<p>正规方程和梯度下降的优缺点，何时使用正规方程：【当n很大时梯度下降】【求n维的矩阵的逆矩阵的代价是三次方】【正规方程适用于线性回归】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131102207082.png" alt="image-20230131102207082"></p>
<h3 id="7-正规方程在矩阵不可以下的解决办法【选学】"><a href="#7-正规方程在矩阵不可以下的解决办法【选学】" class="headerlink" title="7.正规方程在矩阵不可以下的解决办法【选学】"></a>7.正规方程在矩阵不可以下的解决办法【选学】</h3><p>normal equation and non-invertibility 正规方程以及不可逆性</p>
<p>non-invertiable (singular&#x2F;degenerate) 奇异&#x2F;退化矩阵</p>
<hr>
<ol>
<li></li>
</ol>
<p>pinv 伪逆 【即使矩阵没有逆矩阵也能正常工作】</p>
<p>inv 逆 </p>
<p>2.什么情况下会出现矩阵不可逆的情况：</p>
<p>【存在特征线性相关的多余特征】【样本数量小于特征数量】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131104842319.png" alt="image-20230131104842319"></p>
<h2 id="3-logistic回归-分类"><a href="#3-logistic回归-分类" class="headerlink" title="3.logistic回归-分类"></a>3.logistic回归-分类</h2><h3 id="1-分类"><a href="#1-分类" class="headerlink" title="1.分类"></a>1.分类</h3><p>discrete value 离散值</p>
<p>logistic regression logistic回归</p>
<p>binary classification problem 二元分类</p>
<hr>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131110244988.png" alt="image-20230131110244988"></p>
<h3 id="2-假设陈述"><a href="#2-假设陈述" class="headerlink" title="2.假设陈述"></a>2.假设陈述</h3><p>sigmoid function &#x2F; logistic function &#x3D;&#x3D; function g</p>
<p>假设函数：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131110836484.png" alt="image-20230131110836484"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131111030884.png" alt="image-20230131111030884"></p>
<h3 id="3-简化代价函数于梯度下降"><a href="#3-简化代价函数于梯度下降" class="headerlink" title="3.简化代价函数于梯度下降"></a>3.简化代价函数于梯度下降</h3><p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131112046837.png" alt="image-20230131112046837"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131112502230.png" alt="image-20230131112502230"></p>
<h3 id="正则化【应对过拟合"><a href="#正则化【应对过拟合" class="headerlink" title="正则化【应对过拟合"></a>正则化【应对过拟合</h3><p>regularization 正则化</p>
<p>regularization parameter 正则化参数λ  lambda</p>
<hr>
<p>【参数越小函数越平滑】</p>
<p>正则化：【用来缩小每个参数的值】【从1开始到n，没有0】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131160312551.png" alt="image-20230131160312551"></p>
<p>【后面会有方法自动选择λ】</p>
<h3 id="线性回归的正则化【两种方法更新"><a href="#线性回归的正则化【两种方法更新" class="headerlink" title="线性回归的正则化【两种方法更新"></a>线性回归的正则化【两种方法更新</h3><p>梯度下降更新的改变：【和直观理解】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131161544844.png" alt="image-20230131161544844"></p>
<p>【解释：θ0没有惩罚，单独拉出来。其他的求导化简以后就是最后一个式子，就是每次θ都乘以一个小于1的数】</p>
<hr>
<p>正规方程方法的改变：【同时解决了不可逆的问题，一定可逆了就】</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131162054900.png" alt="image-20230131162054900"></p>
<h3 id="logistic回归的正则化"><a href="#logistic回归的正则化" class="headerlink" title="logistic回归的正则化"></a>logistic回归的正则化</h3><p>梯度下降法的更新：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131163302978.png" alt="image-20230131163302978"></p>
<p>【和线性回归的几乎一样，假设函数不一样】</p>
<hr>
<p>高阶的方法的更新：略。</p>
<h2 id="4-神经网络"><a href="#4-神经网络" class="headerlink" title="4.神经网络"></a>4.神经网络</h2><h3 id="1-模型展示1"><a href="#1-模型展示1" class="headerlink" title="1.模型展示1"></a>1.模型展示1</h3><p>neural networks 神经网络</p>
<p>neurons 神经元</p>
<p>dendrites 树突</p>
<p>axon 轴突</p>
<p>a neuron is a computational unit.</p>
<p>spikes 动作电位</p>
<p>logistic unit 逻辑单元【神经元模拟成一个逻辑单元】</p>
<p>bias unit &#x2F; bias neuron 偏置单元&#x2F;偏置神经元【总是为零】</p>
<hr>
<p><code>sigmoid(logistic) activation function 激活函数</code></p>
<p><code>weights of model == 权重【参数】</code></p>
<p><code>first layer == input layer 第一层叫做输入层</code></p>
<p><code>final layer == output layer 最后一层叫做输出层</code></p>
<p><code>hidden layer 中间的叫做隐藏层</code></p>
<p><code>激活项 == 一个具体神经元计算并输出的值</code></p>
<p><code>matrix of weights 权重矩阵</code></p>
<hr>
<p>模型展示&#x2F;假设函数：数学定义</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131205236845.png" alt="image-20230131205236845"></p>
<h3 id="2-模型展示2"><a href="#2-模型展示2" class="headerlink" title="2.模型展示2"></a>2.模型展示2</h3><p>forward propagation 向前传播</p>
<p>vectorized implementation 向量化实现</p>
<hr>
<p>向前传播【向量化】：</p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230131210551742.png" alt="image-20230131210551742"></p>
<hr>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230215225124584.png" alt="image-20230215225124584"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230215225147659.png" alt="image-20230215225147659"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230215225159765.png" alt="image-20230215225159765"></p>
<p><img src="/../images/13ML%E8%AE%B0%E5%BD%95/image-20230215225212492.png" alt="image-20230215225212492"></p>
<h2 id="5-无监督学习-K-Means算法"><a href="#5-无监督学习-K-Means算法" class="headerlink" title="5.无监督学习-K-Means算法"></a>5.无监督学习-K-Means算法</h2>
            </div>

            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/ML/">#ML</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                               rel="prev"
                               href="/2023/02/02/14%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0%E8%AE%B0%E5%BD%95/"
                            >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                                <span class="title flex-center">
                                <span class="post-nav-title-item">漏洞复现记录</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                               rel="next"
                               href="/2023/01/26/12hgame-week2-web%E5%A4%8D%E7%8E%B0%E8%BF%87%E7%A8%8B/"
                            >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">hgame-week2-web复现过程</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                                <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    
<div class="comments-container">
    <div id="comments-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments"></i>&nbsp;Comments
    </div>
    
        
            

    <div class="gitalk-comment-container">
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.css">
        <script  src="//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js"></script>
        <script >
          function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
              __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
              Gitalk && new Gitalk({
                clientID: 'a415960e8a19902e98bf',
                clientSecret: '97151a67ce9c95ad3c333c402f4cee0f5f05020f',
                repo: 'gitalk',
                owner: 'Lejeremiah',
                admin: 'Lejeremiah',
                id: __gitalk__pathname,
                language: 'en'
              }).render('gitalk-container');
            } catch (e) {
              window.Gitalk = null;
            }
          }

          if ('false' === 'true') {
            const loadGitalkTimeout = setTimeout(() => {
              loadGitalk();
              clearTimeout(loadGitalkTimeout);
            }, 1000);
          } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
          }
        </script>
    </div>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ML%E8%AE%B0%E5%BD%95-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86"><span class="nav-text">ML记录-理论基础部分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80%EF%BC%9A"><span class="nav-text">前言：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">1.线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="nav-text">1.模型介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-text">2.代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88%E4%B8%80%EF%BC%89"><span class="nav-text">3.代价函数（一）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88%E4%BA%8C%EF%BC%89"><span class="nav-text">4.代价函数（二）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">5.梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93"><span class="nav-text">6.梯度下降知识总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">7.线性回归的梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%A4%9A%E5%85%83%E6%83%85%E5%86%B5%EF%BC%88%E7%9F%A9%E9%98%B5"><span class="nav-text">2.多元情况（矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%9A%E5%85%83%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0"><span class="nav-text">1.多元假设函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%A4%9A%E5%85%83%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">2.多元梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-%E3%80%90%E5%B8%AE%E5%8A%A9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9B%B4%E5%BF%AB%E7%9A%84%E6%94%B6%E6%95%9B%E3%80%91"><span class="nav-text">3.特征缩放-【帮助梯度下降更快的收敛】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-text">4.学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-text">5.特征和多项式回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%EF%BC%88%E5%8C%BA%E5%88%AB%E4%BA%8E%E8%BF%AD%E4%BB%A3%E6%B3%95"><span class="nav-text">6.正规方程（区别于迭代法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%9C%A8%E7%9F%A9%E9%98%B5%E4%B8%8D%E5%8F%AF%E4%BB%A5%E4%B8%8B%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E3%80%90%E9%80%89%E5%AD%A6%E3%80%91"><span class="nav-text">7.正规方程在矩阵不可以下的解决办法【选学】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-logistic%E5%9B%9E%E5%BD%92-%E5%88%86%E7%B1%BB"><span class="nav-text">3.logistic回归-分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%88%86%E7%B1%BB"><span class="nav-text">1.分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%81%87%E8%AE%BE%E9%99%88%E8%BF%B0"><span class="nav-text">2.假设陈述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%AE%80%E5%8C%96%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E4%BA%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">3.简化代价函数于梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%90%E5%BA%94%E5%AF%B9%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-text">正则化【应对过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%90%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E6%9B%B4%E6%96%B0"><span class="nav-text">线性回归的正则化【两种方法更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">logistic回归的正则化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">4.神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%B1%95%E7%A4%BA1"><span class="nav-text">1.模型展示1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%B1%95%E7%A4%BA2"><span class="nav-text">2.模型展示2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-K-Means%E7%AE%97%E6%B3%95"><span class="nav-text">5.无监督学习-K-Means算法</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2020</span> -
            
            2024
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">jerem1ah</a>
            
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.5.2</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="tools-item flex-center go-to-comments">
                <i class="fas fa-comment"></i>
                <span class="post-comments-count"></span>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>







    
<script src="/js/code-block.js"></script>





<div class="post-scripts">
    
        
<script src="/js/post-helper.js"></script>

        
            
<script src="/js/libs/anime.min.js"></script>

        
        
            
<script src="/js/toc.js"></script>

        
    
</div>



</body>
</html>
